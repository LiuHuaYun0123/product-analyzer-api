import requests
import json
import extruct
import jsonlines
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from w3lib.html import get_base_url
import time
import re  # âœ… æ–°å¢å¯¼å…¥

# === é…ç½® Google CSE ===
API_KEY = 'AIzaSyDqwdySk1md1n3NfnYLwhA5GYgBoSoLjUI'
CX = 'd7a2a5fa7357f49ad'

def google_image_search(query, total_results=30, site_domains=None):
    """
    æ‰©å±•ç‰ˆ Google å›¾ç‰‡æœç´¢ï¼šæ”¯æŒåˆ†é¡µå’ŒæŒ‡å®šç«™ç‚¹ã€‚
    è¿”å›æ€»å…±æœ€å¤š 10 æ¡ç»“æœã€‚
    """
    results = []
    base_url = "https://www.googleapis.com/customsearch/v1"
    num_per_request = 10  # æ¯é¡µæœ€å¤§è¯·æ±‚æ•°ï¼ˆGoogleé™åˆ¶ï¼‰
    domain_list = site_domains or [None]  # å¦‚æœæ²¡ä¼ site_domainsï¼Œåˆ™è·‘å…¨ç½‘æœç´¢

    for domain in domain_list:
        for start in range(1, total_results + 1, num_per_request):
            if len(results) >= 10:  # âœ… é™åˆ¶æ€»å…±è¿”å›æœ€å¤š 10 æ¡
                return results

            params = {
                'key': API_KEY,
                'cx': CX,
                'searchType': 'image',
                'q': query,
                'num': num_per_request,
                'start': start
            }
            if domain:
                params['siteSearch'] = domain
                params['siteSearchFilter'] = 'i'  # include

            try:
                response = requests.get(base_url, params=params, timeout=10)
                data = response.json()

                for item in data.get('items', []):
                    if len(results) >= 10:  # âœ… å†æ¬¡åˆ¤æ–­ä¸Šé™
                        return results
                    image_link = item['link']
                    context_link = item.get('image', {}).get('contextLink')
                    results.append({'image_url': image_link, 'source_url': context_link})

                time.sleep(1)  # é˜²æ­¢ API å°é”
            except Exception as e:
                print(f"âš ï¸ è¯·æ±‚å¤±è´¥: {e} (start={start}, site={domain})")
    return results

def extract_product_info(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=10)
        base_url = get_base_url(resp.text, resp.url)

        # --- ç”¨ extruct æŠ½å–ç»“æ„åŒ–æ•°æ® ---
        metadata = extruct.extract(resp.text, base_url=base_url, syntaxes=['json-ld', 'microdata'], uniform=True)
        product = {}

        for item in metadata.get('json-ld', []):
            if isinstance(item, dict) and item.get('@type') == 'Product':
                product['å•†å“å'] = item.get('name')
                brand = item.get('brand')
                if isinstance(brand, dict):
                    product['ãƒ–ãƒ©ãƒ³ãƒ‰'] = brand.get('name')
                else:
                    product['ãƒ–ãƒ©ãƒ³ãƒ‰'] = brand
                product['è‰²'] = item.get('color')
                product['ç´ æ'] = item.get('material')
                product['ã‚µã‚¤ã‚º'] = item.get('size')
                product['å“ç•ª'] = item.get('sku') or item.get('mpn')
                break  # åªå–ç¬¬ä¸€ä¸ª Product

        # --- å¦‚æœç»“æ„åŒ–æ•°æ®ä¸å…¨ï¼Œå°è¯•ä» HTML ä¸­æå– ---
        soup = BeautifulSoup(resp.text, 'lxml')

        def find_in_text(patterns):
            """
            ç²¾å‡†æå–æ ¼å¼ä¸ºã€Œã‚­ãƒ¼: å€¤ã€æˆ–ã€Œã‚­ãƒ¼ï¼šå€¤ã€çš„å­—æ®µï¼Œé¿å…è¯¯æŠ“å…¬å¸ä¿¡æ¯ç­‰é•¿æ–‡æœ¬ã€‚
            """
            for tag in soup.find_all(['li', 'td', 'th', 'p', 'span', 'div']):
                text = tag.get_text(strip=True)
                for key in patterns:
                    match = re.match(rf'^{re.escape(key)}\s*[:ï¼š]\s*(.+)$', text)
                    if match:
                        value = match.group(1).strip()
                        if len(value) > 100:
                            continue  # å¿½ç•¥å¼‚å¸¸è¿‡é•¿å†…å®¹
                        return value
            return None

        if not product.get('å•†å“å'):
            product['å•†å“å'] = soup.title.string.strip() if soup.title else None

        if not product.get('ãƒ–ãƒ©ãƒ³ãƒ‰'):
            product['ãƒ–ãƒ©ãƒ³ãƒ‰'] = find_in_text(['ãƒ–ãƒ©ãƒ³ãƒ‰', 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'Brand'])

        if not product.get('è‰²'):
            product['è‰²'] = find_in_text(['ã‚«ãƒ©ãƒ¼', 'è‰²', 'Color'])

        if not product.get('ç´ æ'):
            product['ç´ æ'] = find_in_text(['ç´ æ', 'Material'])

        if not product.get('ã‚µã‚¤ã‚º'):
            product['ã‚µã‚¤ã‚º'] = find_in_text(['ã‚µã‚¤ã‚º', 'Size'])

        if not product.get('é‡‘å…·'):
            product['é‡‘å…·'] = find_in_text(['é‡‘å…·', 'Hardware'])

        if not product.get('å“ç•ª'):
            product['å“ç•ª'] = find_in_text(['å“ç•ª', 'å‹ç•ª', 'SKU', 'å•†å“ç•ªå·'])

        # âœ… æœ€åç»Ÿä¸€è¡¥å…¨å­—æ®µ
        required_fields = ['å•†å“å', 'ãƒ–ãƒ©ãƒ³ãƒ‰', 'è‰²', 'ç´ æ', 'ã‚µã‚¤ã‚º', 'é‡‘å…·', 'å“ç•ª']
        for field in required_fields:
            if field not in product or product[field] is None:
                product[field] = "null"

        return product

    except Exception as e:
        return {'error': str(e)}

def main():
    queries = [
        "SALE ç¾å“ GUCCI ã‚°ãƒƒãƒ ãƒ•ãƒ­ãƒ¼ãƒ© ã‚¯ãƒªã‚¢ãƒãƒƒã‚° 548713 ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹ ãƒˆãƒ¼ãƒˆãƒãƒƒã‚°",
    ]

    site_list = [
        None,
        "www.buyma.com",
        "www.farfetch.com",
        "item.rakuten.co.jp",
        "www.yoox.com",
    ]

    with jsonlines.open('results.jsonl', mode='w') as writer:
        for query in queries:
            print(f"ğŸ” æ¤œç´¢: {query}")
            try:
                search_results = google_image_search(query, total_results=30, site_domains=site_list)
                print(f"ğŸ“¸ æ‰¾åˆ°å›¾ç‰‡æ•°é‡: {len(search_results)}")
                for result in search_results:
                    source_url = result.get('source_url')
                    if not source_url:
                        print("âš ï¸ ç¼ºå°‘ source_urlï¼Œè·³è¿‡")
                        continue
                    print(f"  ğŸŒ å–å¾—ä¸­: {source_url}")
                    info = extract_product_info(source_url)
                    print("âœ… æŠ½å–ç»“æœ:", info)
                    writer.write({
                        'query': query,
                        'image_url': result['image_url'],
                        'source_url': source_url,
                        'product_info': info
                    })
                    time.sleep(1)
            except Exception as e:
                print(f"âŒ æœç´¢æˆ–å†™å…¥é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
